{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Finding the Sentence Similarity between two sentences"]},{"cell_type":"markdown","metadata":{},"source":["Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. \n","\n","### Definition\n","It is defined to equal the cosine of the angle between them, which is also the same as the inner product of the same vectors normalized to both have length 1."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:15.669800Z","iopub.status.busy":"2024-02-15T12:26:15.669066Z","iopub.status.idle":"2024-02-15T12:26:17.489127Z","shell.execute_reply":"2024-02-15T12:26:17.487959Z","shell.execute_reply.started":"2024-02-15T12:26:15.669743Z"},"trusted":true},"outputs":[],"source":["#importing libraries\n","\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.491125Z","iopub.status.busy":"2024-02-15T12:26:17.490827Z","iopub.status.idle":"2024-02-15T12:26:17.500473Z","shell.execute_reply":"2024-02-15T12:26:17.498930Z","shell.execute_reply.started":"2024-02-15T12:26:17.491097Z"},"trusted":true},"outputs":[],"source":["#Finding the cosine similarity between two sentences X and Y\n","\n","X = (\"Hi! How are you ?\").lower()\n","Y = (\"Are you from Banglore ?\").lower()"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenization\n","Tokenization is a common task in Natural Language Processing (NLP).\n","Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.502589Z","iopub.status.busy":"2024-02-15T12:26:17.502237Z","iopub.status.idle":"2024-02-15T12:26:17.529732Z","shell.execute_reply":"2024-02-15T12:26:17.528584Z","shell.execute_reply.started":"2024-02-15T12:26:17.502547Z"},"trusted":true},"outputs":[],"source":["#Tokenize the given sentences into tokens.\n","X_list = word_tokenize(X)  \n","Y_list = word_tokenize(Y) "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.531496Z","iopub.status.busy":"2024-02-15T12:26:17.531104Z","iopub.status.idle":"2024-02-15T12:26:17.538568Z","shell.execute_reply":"2024-02-15T12:26:17.537673Z","shell.execute_reply.started":"2024-02-15T12:26:17.531465Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(['hi', '!', 'how', 'are', 'you', '?'],\n"," ['are', 'you', 'from', 'banglore', '?'])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["X_list, Y_list"]},{"cell_type":"markdown","metadata":{},"source":["### What are stop words?\n","Stopwords are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as “The Who” or “Take That”."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.542058Z","iopub.status.busy":"2024-02-15T12:26:17.541677Z","iopub.status.idle":"2024-02-15T12:26:17.552059Z","shell.execute_reply":"2024-02-15T12:26:17.551076Z","shell.execute_reply.started":"2024-02-15T12:26:17.542023Z"},"trusted":true},"outputs":[],"source":["#printing the stopwords in english\n","sw = stopwords.words('english')  \n","l1 =[];l2 =[] "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.553719Z","iopub.status.busy":"2024-02-15T12:26:17.553290Z","iopub.status.idle":"2024-02-15T12:26:17.561770Z","shell.execute_reply":"2024-02-15T12:26:17.560915Z","shell.execute_reply.started":"2024-02-15T12:26:17.553688Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["sw"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.563531Z","iopub.status.busy":"2024-02-15T12:26:17.563213Z","iopub.status.idle":"2024-02-15T12:26:17.572767Z","shell.execute_reply":"2024-02-15T12:26:17.571591Z","shell.execute_reply.started":"2024-02-15T12:26:17.563502Z"},"trusted":true},"outputs":[],"source":["## remove stop words from the string \n","X_set = {w for w in X_list if not w in sw}  \n","Y_set = {w for w in Y_list if not w in sw} "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.574552Z","iopub.status.busy":"2024-02-15T12:26:17.574240Z","iopub.status.idle":"2024-02-15T12:26:17.584708Z","shell.execute_reply":"2024-02-15T12:26:17.583755Z","shell.execute_reply.started":"2024-02-15T12:26:17.574523Z"},"trusted":true},"outputs":[{"data":{"text/plain":["({'!', '?', 'hi'}, {'?', 'banglore'})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_set, Y_set"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:26:17.586365Z","iopub.status.busy":"2024-02-15T12:26:17.585968Z","iopub.status.idle":"2024-02-15T12:26:17.596089Z","shell.execute_reply":"2024-02-15T12:26:17.594917Z","shell.execute_reply.started":"2024-02-15T12:26:17.586335Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["similarity:  0.4082482904638631\n"]}],"source":["# form a set containing keywords of both strings  \n","rvector = X_set.union(Y_set)  \n","for w in rvector: \n","    if w in X_set: l1.append(1) # create a vector \n","    else: l1.append(0) \n","    if w in Y_set: l2.append(1) \n","    else: l2.append(0) \n","c = 0\n","  \n","# cosine formula  \n","for i in range(len(rvector)): \n","        c+= l1[i]*l2[i] \n","cosine = c / float((sum(l1)*sum(l2))**0.5) \n","print(\"similarity: \", cosine)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30004,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
